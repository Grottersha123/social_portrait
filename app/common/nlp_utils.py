import re
from functools import reduce
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from pymystem3 import Mystem
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download("stopwords")
from nltk.corpus import stopwords
# TODO hash tag оставляем
additional_stopwords = ["еще", "ещё", "меж", "зато", "пусть", "ага", "этот", "это", "почему",
                        "весь", "ты", "он", "она", "они", "оно", "мы", "вы", "кто", "что",
                        "сам", "сама", "само", "свой", "наш", "ваш", "их", "тот", "та", "те",
                        "то", "раз", "твой", "мой", "кой", "кое", "все", "весь", "всё", "быть", "тот",
                        "таки", "такой", "какой", "каждый", "который", "и", "а", "в", "б", "д",
                        "е", "ж", "з", "к", "л", "м", "н", "о", "п", "р", "с", "у", "ф", "ч",
                        "ц", "ш", "щ", "ь", "ъ","э", "ю", "я"]
stop_words_ru = ["c","а","алло","без","белый","близко","более","больше","большой","будем","будет","будете","будешь","будто","буду","будут","будь","бы","бывает","бывь","был","была","были","было","быть","в","важная","важное","важные","важный","вам","вами","вас","ваш","ваша","ваше","ваши","вверх","вдали","вдруг","ведь","везде","вернуться","весь","вечер","взгляд","взять","вид","видел","видеть","вместе","вне","вниз","внизу","во","вода","война","вокруг","вон","вообще","вопрос","восемнадцатый","восемнадцать","восемь","восьмой","вот","впрочем","времени","время","все","все еще","всегда","всего","всем","всеми","всему","всех","всею","всю","всюду","вся","всё","второй","вы","выйти","г","где","главный","глаз","говорил","говорит","говорить","год","года","году","голова","голос","город","да","давать","давно","даже","далекий","далеко","дальше","даром","дать","два","двадцатый","двадцать","две","двенадцатый","двенадцать","дверь","двух","девятнадцатый","девятнадцать","девятый","девять","действительно","дел","делал","делать","делаю","дело","день","деньги","десятый","десять","для","до","довольно","долго","должен","должно","должный","дом","дорога","друг","другая","другие","других","друго","другое","другой","думать","душа","е","его","ее","ей","ему","если","есть","еще","ещё","ею","её","ж","ждать","же","жена","женщина","жизнь","жить","за","занят","занята","занято","заняты","затем","зато","зачем","здесь","земля","знать","значит","значить","и","иди","идти","из","или","им","имеет","имел","именно","иметь","ими","имя","иногда","их","к","каждая","каждое","каждые","каждый","кажется","казаться","как","какая","какой","кем","книга","когда","кого","ком","комната","кому","конец","конечно","которая","которого","которой","которые","который","которых","кроме","кругом","кто","куда","лежать","лет","ли","лицо","лишь","лучше","любить","люди","м","маленький","мало","мать","машина","между","меля","менее","меньше","меня","место","миллионов","мимо","минута","мир","мира","мне","много","многочисленная","многочисленное","многочисленные","многочисленный","мной","мною","мог","могу","могут","мож","может","может быть","можно","можхо","мои","мой","мор","москва","мочь","моя","моё","мы","на","наверху","над","надо","назад","наиболее","найти","наконец","нам","нами","народ","нас","начала","начать","наш","наша","наше","наши","не","него","недавно","недалеко","нее","ней","некоторый","нельзя","нем","немного","нему","непрерывно","нередко","несколько","нет","нею","неё","ни","нибудь","ниже","низко","никакой","никогда","никто","никуда","ним","ними","них","ничего","ничто","но","новый","нога","ночь","ну","нужно","нужный","нх","о","об","оба","обычно","один","одиннадцатый","одиннадцать","однажды","однако","одного","одной","оказаться","окно","около","он","она","они","оно","опять","особенно","остаться","от","ответить","отец","откуда","отовсюду","отсюда","очень","первый","перед","писать","плечо","по","под","подойди","подумать","пожалуйста","позже","пойти","пока","пол","получить","помнить","понимать","понять","пор","пора","после","последний","посмотреть","посреди","потом","потому","почему","почти","правда","прекрасно","при","про","просто","против","процентов","путь","пятнадцатый","пятнадцать","пятый","пять","работа","работать","раз","разве","рано","раньше","ребенок","решить","россия","рука","русский","ряд","рядом","с","с кем","сам","сама","сами","самим","самими","самих","само","самого","самой","самом","самому","саму","самый","свет","свое","своего","своей","свои","своих","свой","свою","сделать","сеаой","себе","себя","сегодня","седьмой","сейчас","семнадцатый","семнадцать","семь","сидеть","сила","сих","сказал","сказала","сказать","сколько","слишком","слово","случай","смотреть","сначала","снова","со","собой","собою","советский","совсем","спасибо","спросить","сразу","стал","старый","стать","стол","сторона","стоять","страна","суть","считать","т","та","так","такая","также","таки","такие","такое","такой","там","твои","твой","твоя","твоё","те","тебе","тебя","тем","теми","теперь","тех","то","тобой","тобою","товарищ","тогда","того","тоже","только","том","тому","тот","тою","третий","три","тринадцатый","тринадцать","ту","туда","тут","ты","тысяч","у","увидеть","уж","уже","улица","уметь","утро","хороший","хорошо","хотел бы","хотеть","хоть","хотя","хочешь","час","часто","часть","чаще","чего","человек","чем","чему","через","четвертый","четыре","четырнадцатый","четырнадцать","что","чтоб","чтобы","чуть","шестнадцатый","шестнадцать","шестой","шесть","эта","эти","этим","этими","этих","это","этого","этой","этом","этому","этот","эту","я","являюсь"]
mystem = Mystem()

def filtering_words(text):
    """

    :type text: lst
    """
    # 'training' (tf-)idf vectorizer.
    tf_idf = TfidfVectorizer(
                             smooth_idf=False
                             )
    tf_idf.fit(text)
    # getting idfs
    idfs = tf_idf.idf_
    # sorting out too rare and too common words
    # original 1.3 and 7
    # 2 6
    lower_thresh = 3.
    upper_thresh = 6.
    not_often = idfs > lower_thresh
    not_rare = idfs < upper_thresh

    mask = not_often * not_rare

    good_words = np.array(tf_idf.get_feature_names())[mask]
    # deleting punctuation as well.
    cleaned = []
    for word in good_words:
        word = re.sub("^(\d+\w*$|_+)", "", word)

        if len(word) == 0:
            continue
        cleaned.append(word)

    print("Len of original vocabulary: %d\nAfter filtering: %d" % (idfs.shape[0], len(cleaned)))
    return cleaned

def preprocess_text(text):
    text = text.lower()
    text = text.replace('ё', 'e')
    patterns = "[0-9!$%&'()*+,./:;<=>#?!@[\]^_`{|}~—\"\-]+\s+"
    text = text.replace('\n', '').replace('\r', '')
    text = re.sub(r'https*\S+', ' ', text)
    regex = re.compile('[^a-zA-Za-яА-Я]')
    # patterns = "[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\]^_`{|}~—\"\-]+"
    text = re.sub(patterns, ' ', text)
    text = re.sub(regex, ' ', text)
    text = re.sub(r'\s+', ' ', text)
    word_tokens = word_tokenize(text)
    stop_words = set(stopwords.words("russian") + additional_stopwords + stop_words_ru)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    print(filtered_text)
    return ' '.join(filtered_text) if len(filtered_text) > 3 else None

def lemmatization(text, num=10):
    lol = lambda lst, sz: [lst[i:i + sz] for i in range(0, len(lst), sz) ]
    listmerge = lambda s: reduce(lambda d,el: d.extend(el) or d, s, [])
    txtpart = lol(text, num)
    res = []
    for txtp in txtpart:
        alltexts = ' '.join([txt.strip() + ' br ' for txt in txtp if not txt  is None])
        print(alltexts)
        words = mystem.lemmatize(alltexts)
        words = ''.join(words)
        res.append(words.split('br'))
    return listmerge(res)


if __name__ == '__main__':
    pass
